{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae14e54-5cf8-4e53-9cf9-2f7c60e16e19",
   "metadata": {},
   "source": [
    "# Assignment Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f2bb7-3f11-4f1f-b32b-71a57a3b720a",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749e5429-516f-47ed-8801-97beee76b78c",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- R-Squared is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. \n",
    "- It r-squared shows how well the data fit the regression model (the goodness of fit).\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "- R-squared is calculated using the formula given below:<br>\n",
    "    - R-squared = 1 - (SS-residuals/SS-total)\n",
    "    - where, SS-residuals = Sum of square of errors; SS-total = Sum of squares of y value from its mean.\n",
    "- It represents the accuracy of the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57815a1a-a8b9-47a8-9c96-f07dc0a80531",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51396c58-98f9-470c-bd71-dc61901cf48f",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. \n",
    "- The adjusted R-squared increases when the new term improves the model more than would be expected by chance. - It decreases when a predictor improves the model by less than expected. Typically, the adjusted R-squared is positive, not negative. It is always lower than the R-squared.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "- The main distinction between adjusted R-squared and R-squared is that the former takes into account and evaluates various independent variables against the stock index, whereas the latter does not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40437054-9d5c-4442-83df-b6ce4521fdb2",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d41626e-c335-4d53-96da-549132d1a018",
   "metadata": {},
   "source": [
    "- Adjusted R-squared can be used when there is large set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3329e515-dab8-4284-9da1-fa8d6fbdf965",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39318d-ddc8-4c6e-890e-11ebd8594b4e",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- RMSE, MSE,and MAE are functions that is used to calculate the errors in regression analysis.\n",
    "- These are called as cost function as they denote the extent of cost incurred in terms of error in the model.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "- MSE is calculated by taking the average of the square of the difference between the original and predicted values of the data.\n",
    "- MAE takes the average of the absolute difference between the actual or true values and the values that are predicted from every sample in a dataset and gives the output.\n",
    "- RMSE is the standard deviation of the errors which occur when a prediction is made on a dataset. This is the same as MSE but the root of the value is considered while determining the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bca30b-d428-4757-b9c3-24d0202d0395",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4605b69-4999-40b6-bbde-cf647f8a0daa",
   "metadata": {},
   "source": [
    "1. RMSE<br>\n",
    "Advantage\n",
    "- It is computationally simple and easily differentiable which many optimization algorithms desire.\n",
    "- It does not penalize the errors as much as MSE does due to the square root.\n",
    "<br>\n",
    "Disadvantage\n",
    "- Like MSE, RMSE is dependent on the scale of the data. It increases in magnitude if the scale of the error increases.\n",
    "- One major drawback of RMSE is its sensitivity to outliers and the outliers have to be removed for it to function properly.\n",
    "- RMSE increases with an increase in the size of the test sample. This is an issue when we calculate the results on different test samples.\n",
    "2. MSE<br>\n",
    "Advantage\n",
    "- The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, since the MSE puts larger weight on theses errors due to the squaring part of the function.\n",
    "<br>\n",
    "Disadvantage\n",
    "- If our model makes a single very bad prediction, the squaring part of the function magnifies the error. \n",
    "3. MAE<br>\n",
    "Advantage\n",
    "- Since we are taking the absolute value, all of the errors will be weighted on the same linear scale. Thus, unlike the MSE, we won’t be putting too much weight on our outliers and our loss function provides a generic and even measure of how well our model is performing.\n",
    "<br>\n",
    "Disadvantage\n",
    "- If we do in fact care about the outlier predictions of our model, then the MAE won’t be as effective. The large errors coming from the outliers end up being weighted the exact same as lower errors. This might results in our model being great most of the time, but making a few very poor predictions every so-often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e7ff26-cf5f-446e-af55-66caed8df981",
   "metadata": {},
   "source": [
    "# 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa527fe2-3d7f-456f-9225-351b374a74e2",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- Lasso regression is a regularization technique. \n",
    "- It is used over regression methods for a more accurate prediction. \n",
    "- This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. \n",
    "- The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). \n",
    "- This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.\n",
    "- Lasso Regression uses L1 regularization technique (will be discussed later in this article). It is used when we have more features because it automatically performs feature selection.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "- Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square.\n",
    "<br><br>\n",
    "##### Part-3:<br><br>\n",
    "- Lasso tends to do well if there are a small number of significant parameters and the others are close to zero ( when only a few predictors actually influence the response). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7742a-560d-4c67-835a-210be1ff8d9d",
   "metadata": {},
   "source": [
    "# 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af83bc-797b-442c-828b-382e51ec4bcb",
   "metadata": {},
   "source": [
    "- Overfitting occurs because a model fails to generalize the data that contains a lot of irrelevant data points. Data points that do not reflect the properties of the data are considered to be irrelevant. An example is noise. \n",
    "- Regularization is the answer to overfitting. It is a technique that improves model accuracy as well as prevents the loss of important data due to underfitting.\n",
    "- Regularization is a technique that adds information to a model to prevent the occurrence of overfitting. It is a type of regression that minimizes the coefficient estimates to zero to reduce the capacity (size) of a model. In this context, the reduction of the capacity of a model involves the removal of extra weights.\n",
    "- It removes extra weights from the selected features and redistributes the weights evenly. This means that regularization discourages the learning of a model of both high complexity and flexibility. A highly flexible model is one that possesses the freedom to fit as many data points as possible.\n",
    "- A model with a lot of features to learn from is at a greater risk of overfitting. By discouraging the learning of (or use of) highly complex and flexible models, the risk of overfitting is lowered.\n",
    "- Example:<br><br>\n",
    "Let’s use a linear regression equation to explain regularization further.<br>\n",
    "\n",
    "Y=B0+B1X1+B2X2+…+BpXp<br>\n",
    "Here, <br>\n",
    "Y represents the value that is to be predicted. Bi stands for the regressor coefficient estimates for the corresponding predictor Xi.<br>\n",
    "Xi represents the weights or magnitudes assigned to various predictors (independent variables). Here, i represents any value greater than or equal to 0, and less than p.<br><br>\n",
    "\n",
    "A loss function is involved in the fitting process. It is computed as the difference between the actual and predicted output from a model. A loss function provides a means of assessing how well an algorithm models given data. It is used to minimize the error, in turn optimizing the weights. In this context, the loss function is referred to as the residual sum of squares (RSS).<br>\n",
    "Below is the equation for the loss function.<br>\n",
    "RSS=∑i=1n(yi–B0−∑j=1pBjxij)^2\n",
    "Based on the training data, the loss function will adjust the coefficients. If the presence of noise or outliers is found in the training data, the approximated coefficients will not generalize well to the unseen data. Regularization comes into play and shrinks the learned estimates towards zero.\n",
    "<br>\n",
    "In other words, it tunes the loss function by adding a penalty term, that prevents excessive fluctuation of the coefficients. Thereby, reducing the chances of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285a9ca-42d1-4671-af4f-5cec70765e40",
   "metadata": {},
   "source": [
    "# 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd25b076-67d1-4935-8204-c3f9cff44be2",
   "metadata": {},
   "source": [
    "While regularized linear models are effective and widely used for regression analysis, they do have some limitations that should be considered before using them in any given scenario.\n",
    "<br><br>\n",
    "1. Limited Interpretability: Regularized linear models can be less interpretable compared to non-regularized models, especially when the regularization is strong. This is because regularization may shrink some coefficients to very small values, or even to zero, effectively removing them from the model. The resulting model may be more challenging to interpret, and it may be difficult to determine which features are most important in predicting the target variable.\n",
    "\n",
    "2. Model Selection: Regularization requires the choice of a regularization parameter, which controls the amount of regularization applied to the model. The optimal value of this parameter is often determined using cross-validation, which can be computationally expensive and time-consuming. In some cases, the optimal parameter value may be difficult to determine, leading to suboptimal model performance.\n",
    "\n",
    "3. Assumption of Linearity: Regularized linear models assume that the relationship between the predictor variables and the target variable is linear. However, this assumption may not always hold in real-world scenarios. Non-linear relationships between variables may require more complex models to accurately capture the relationship.\n",
    "\n",
    "4. Limited Ability to Capture Complex Interactions: Regularized linear models are limited in their ability to capture complex interactions between variables. While interactions can be included in the model, this increases the number of parameters and can make the model more complex and difficult to interpret.\n",
    "\n",
    "5. Outliers: Regularized linear models can be sensitive to outliers, as the regularization penalty may not be sufficient to offset the effect of a large outlier on the model. In such cases, it may be necessary to either remove the outliers or use a different model that is more robust to outliers.\n",
    "<br><br>\n",
    "In summary, while regularized linear models are useful and widely used for regression analysis, they may not always be the best choice. It is important to carefully consider the specific limitations of these models, as well as the assumptions they make about the relationship between variables, before using them in any given scenario. Other regression techniques, such as decision trees or neural networks, may be better suited for certain types of data or problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1172db8-0ac5-4f57-8b87-759b7e792cf7",
   "metadata": {},
   "source": [
    "# 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75efda0-d30b-4d51-aea9-6a274db53c3c",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "Given that Model A has RMSE value of 10 and Model B has MAE value of 8.<br>\n",
    "This shows that Model B has average preicted value more near to its actual value. While the large value of RMSE here may denote that there are some value which are overestimated by Model A.<br>\n",
    "Therefore, on the basis of above observation we can say that Model B has better performance than Model A.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "Here, we have choosen MAE metric's value to determine the performance of Model B. But, it is not always good as it doesn't give the good description about the predicted values and error associated with them when there is outlier present in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95baaa9-7ac0-4b40-b165-2be85cc89290",
   "metadata": {},
   "source": [
    "# 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2b4b3-a760-493a-ba81-04b4d9f7c5ff",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "The choice of the better performing model depends on the specific problem and the data at hand. However, some general considerations can be made regarding the differences between Ridge and Lasso regularization.\n",
    "\n",
    "Ridge regularization adds a penalty term proportional to the squared magnitude of the coefficients to the objective function being optimized. This penalty tends to drive the coefficients towards zero, but not necessarily to exactly zero. As a result, Ridge regularization can be effective when there are many features with small to medium effect sizes, as it will shrink all of the coefficients to some extent without completely eliminating any of them. A regularization parameter of 0.1 is relatively mild, and should result in only moderate regularization.\n",
    "\n",
    "Lasso regularization, on the other hand, adds a penalty term proportional to the absolute magnitude of the coefficients to the objective function. This penalty has a more pronounced effect on smaller coefficients, and can lead to some coefficients being exactly zero. As a result, Lasso regularization is particularly effective when there are many features with small effect sizes, as it can eliminate some of the less important features entirely. A regularization parameter of 0.5 is relatively strong, and should result in significant regularization.\n",
    "\n",
    "Based on these considerations, it is possible that Model B with Lasso regularization is the better performing model, particularly if there are many features with small effect sizes that can be eliminated entirely. However, if all features are believed to be important and have similar effect sizes, Ridge regularization may be more appropriate.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "There are trade-offs and limitations to both regularization methods. Ridge regularization tends to produce smoother coefficient estimates, and is less likely to lead to overfitting in high-dimensional datasets. However, it may not be as effective at feature selection as Lasso regularization. Lasso regularization is better at feature selection, but can produce more variable coefficient estimates, and may be more prone to overfitting in high-dimensional datasets. Additionally, the choice of the regularization parameter is important, as a value that is too high can result in excessive regularization and a model that is too simple, while a value that is too low can result in overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d073c1-50aa-4bdc-8b2a-55f74105e426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
