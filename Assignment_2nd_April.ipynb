{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0028bb6-6bbd-4ce3-914e-07906edf807c",
   "metadata": {},
   "source": [
    "# Assignment Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd7dcd-4331-4a3d-887c-7a10293b13b4",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df78d5-78a1-44bf-8190-5ff281240204",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- Grid Search CV (Cross-Validation) is a technique used in machine learning to search for the best combination of hyperparameters of a model that gives the best performance on a particular dataset. \n",
    "- Hyperparameters are the model parameters that cannot be learned from the data but are set before training the model. \n",
    "- Grid Search CV exhaustively searches through a pre-defined hyperparameter grid, evaluating the performance of the model with each hyperparameter combination using cross-validation, and returns the combination that gives the best performance.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "The process of Grid Search CV involves the following steps:\n",
    "\n",
    "1. Defining the model: \n",
    "- The first step is to define the model that you want to tune. \n",
    "- This includes selecting the algorithm and its hyperparameters.\n",
    "\n",
    "2. Defining the hyperparameter grid: \n",
    "- The next step is to define the hyperparameter grid to search over. \n",
    "- The grid should include all possible combinations of hyperparameters to test.\n",
    "\n",
    "3. Cross-validation: \n",
    "- Grid Search CV then performs a k-fold cross-validation on each hyperparameter combination in the grid, evaluating the model's performance on each fold.\n",
    "\n",
    "4. Evaluation: \n",
    "- The performance of each hyperparameter combination is then evaluated using a scoring metric such as accuracy, precision, recall, F1-score, or ROC AUC score.\n",
    "\n",
    "5. Selecting the best hyperparameters: \n",
    "- Finally, the hyperparameters that give the best performance are selected and used to train the final model.\n",
    "<br><br>\n",
    "Grid Search CV is a computationally intensive process as it involves training and evaluating the model for every possible combination of hyperparameters. However, it is an essential technique for optimizing the performance of machine learning models and finding the best hyperparameter values for a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26eb09a-fe63-496e-b9a7-93e9299c76e1",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a87c7e-c920-467a-827e-87767ee83bc9",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- Grid search cv and randomized search cv are both hyperparameter tuning techniques used to optimize a machine learning model.\n",
    "\n",
    "- Grid search cv exhaustively searches through a specified range of hyperparameters by testing all possible combinations of hyperparameters. It creates a grid of hyperparameters and evaluates the model performance using cross-validation on each point of the grid. It selects the combination of hyperparameters that yield the best performance score.\n",
    "\n",
    "- Randomized search cv, on the other hand, randomly selects hyperparameters from a specified range of values. It performs a set number of iterations and selects the best hyperparameters based on the performance score. Randomized search cv is generally faster than grid search cv because it doesn't test every possible combination of hyperparameters.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "The choice between grid search cv and randomized search cv depends on the size of the hyperparameter search space and the computational resources available. Grid search cv is suitable for smaller hyperparameter search spaces, whereas randomized search cv is more efficient for larger search spaces. If computational resources are limited, randomized search cv may be a better option as it can produce similar results to grid search cv with fewer iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ffc23-139d-4c00-a6e1-08bc60065ad7",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a0e1f-87a1-400f-863e-6d7e8e31883f",
   "metadata": {},
   "source": [
    "- Data leakage is a problem in machine learning that occurs when information from the test set is inadvertently included in the training set, leading to overly optimistic performance estimates and poor generalization to new, unseen data.\n",
    "<br><br>\n",
    "An example of data leakage could be in the case of a credit scoring model. If the model is trained on data that includes the credit limit of a customer, then the model may learn that customers with higher credit limits are more likely to repay their debts, leading to overfitting to this particular feature. If the model is then used to make predictions on new customers, who may have different credit limits, the performance may be poor due to the model's reliance on this feature during training. This is because the credit limit is not an intrinsic characteristic of the customer, but rather a consequence of the loan decision, and as such is not available at the time of prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a420c6-75b4-4c98-96a8-4111f50516e0",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805235c7-4c7a-4548-ae99-091648f7f38f",
   "metadata": {},
   "source": [
    "Data leakage can be prevented by following some best practices in data preparation and modeling. Here are some ways to prevent data leakage:\n",
    "<br>\n",
    "1. Separate data into training and validation sets: \n",
    "- Always split the data into training and validation sets before performing any preprocessing or modeling steps. \n",
    "- This helps to ensure that the model is trained and evaluated on different data, reducing the risk of overfitting.\n",
    "\n",
    "2. Perform feature selection and engineering on the training data: \n",
    "- Feature selection and engineering should only be performed on the training data to avoid introducing any information from the validation or test sets into the model.\n",
    "\n",
    "3. Use cross-validation: \n",
    "- Cross-validation is a technique that involves splitting the data into multiple subsets and training and testing the model on different combinations of these subsets. \n",
    "- This helps to reduce the risk of overfitting and ensures that the model generalizes well to new data.\n",
    "\n",
    "4. Be cautious with data preprocessing: \n",
    "- Data preprocessing steps such as normalization or scaling should be performed separately on the training and validation sets. \n",
    "- In addition, we need to be careful when imputing missing values or encoding categorical variables, as these steps can introduce information from the validation or test sets into the model.\n",
    "\n",
    "5. Avoid using future information: \n",
    "- Any information that is not available at the time of prediction should not be used in the model. \n",
    "- For example, if we are building a model to predict stock prices, we should not include information about the stock prices from the future in the training data.\n",
    "\n",
    "6. Use pipelines: \n",
    "- Pipelines can help to ensure that data preprocessing and modeling steps are performed in the correct order and that data leakage is minimized.\n",
    "<br><br>\n",
    "By following these best practices, we can reduce the risk of data leakage and ensure that your machine learning model is robust and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efb991d-b310-4c75-9369-eda6a8db287e",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a8d24-1acc-46ae-b543-5c6aa0ee11e7",
   "metadata": {},
   "source": [
    "- A confusion matrix is a table used to evaluate the performance of a classification model by comparing the predicted values against the actual values. \n",
    "- It displays the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) for each class in the classification problem.\n",
    "- Each of the afore-mentioned results enable us to judge the working of the model under different conditions.\n",
    "- Importance of each of these factors are mentioned below\n",
    "<br><br>\n",
    "True Positive (TP): The number of correctly predicted positive cases.\n",
    "False Positive (FP): The number of incorrectly predicted positive cases.\n",
    "True Negative (TN): The number of correctly predicted negative cases.\n",
    "False Negative (FN): The number of incorrectly predicted negative cases.\n",
    "<br><br>\n",
    "From the confusion matrix, various performance metrics can be calculated, such as accuracy, precision, recall, and F1-score. These metrics help to evaluate how well the model is performing and identify any potential issues or areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7563ee62-2f61-44a4-9fd9-dbb9355a4009",
   "metadata": {},
   "source": [
    "# 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc56abe9-161e-44ad-b351-80efdbb80871",
   "metadata": {},
   "source": [
    "- Precision is a measure of how many of the predicted positive instances are actually positive. It is the ratio of true positives (TP) to the sum of true positives and false positives (FP). In other words, precision measures how precise or accurate the model is when it predicts a positive instance.\n",
    "\n",
    "- Recall, on the other hand, is a measure of how many of the actual positive instances are correctly identified by the model. It is the ratio of true positives to the sum of true positives and false negatives (FN). In other words, recall measures how well the model can identify positive instances out of all the actual positive instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b784f53-be1d-4931-aaeb-4fb3445319bc",
   "metadata": {},
   "source": [
    "# 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e82ba-e2fb-4852-a354-6052d117dae5",
   "metadata": {},
   "source": [
    "To interpret a confusion matrix and determine which types of errors your model is making, we need to look at the values in each of the cells. The main metrics that are commonly derived from the confusion matrix are precision, recall, and accuracy.\n",
    "\n",
    "1. Precision is the fraction of correct positive predictions among all positive predictions. It is calculated as TP/(TP + FP). Precision tells us how well our model identifies positive samples correctly.\n",
    "\n",
    "2. Recall, also called sensitivity or true positive rate, is the fraction of positive samples that are correctly identified as positive. It is calculated as TP/(TP + FN). Recall tells us how well our model detects positive samples.\n",
    "\n",
    "3. Accuracy is the fraction of correctly classified samples among all samples. It is calculated as (TP + TN)/(TP + TN + FP + FN). Accuracy is a general measure of the model's performance.\n",
    "\n",
    "- By examining the values in the confusion matrix, we can identify the types of errors our model is making. \n",
    "- For example, \n",
    "    - if there are many false positives (high FP rate), our model may be too aggressive in classifying samples as positive that is type-1 error.\n",
    "    - If there are many false negatives (high FN rate), our model may be missing many positive samples that is type-2 error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4b899-887b-42f7-8556-47bfd5a7e3eb",
   "metadata": {},
   "source": [
    "# 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d246d1-d7ec-4c57-ad95-d496b9f18ade",
   "metadata": {},
   "source": [
    "1. Accuracy: \n",
    "- It measures the overall performance of the model. \n",
    "- It is calculated as the ratio of correct predictions to the total number of predictions.\n",
    "<br><br>\n",
    "Accuracy = (True Positive + True Negative) / (True Positive + False Positive + True Negative + False Negative)\n",
    "\n",
    "2. Precision: \n",
    "- It measures how many of the predicted positive instances are actually positive. \n",
    "- It is calculated as the ratio of true positives to the sum of true positives and false positives.\n",
    "<br><br>\n",
    "Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "3. Recall (Sensitivity): \n",
    "- It measures how many of the actual positive instances were predicted as positive. It is calculated as the ratio of true positives to the sum of true positives and false negatives.\n",
    "<br><br>\n",
    "Recall = True Positive / (True Positive + False Negative)\n",
    "\n",
    "4. Specificity: \n",
    "- It measures how many of the actual negative instances were predicted as negative. \n",
    "- It is calculated as the ratio of true negatives to the sum of true negatives and false positives.\n",
    "<br><br>\n",
    "Specificity = True Negative / (True Negative + False Positive)\n",
    "\n",
    "5. F1 Score: \n",
    "- It is the harmonic mean of precision and recall. \n",
    "- It is calculated as 2 times the product of precision and recall divided by their sum.\n",
    "<br><br>\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "6. AUC-ROC (Area Under the Receiver Operating Characteristic Curve): \n",
    "- It measures the model's ability to distinguish between positive and negative classes. \n",
    "- It is calculated as the area under the ROC curve.\n",
    "- The ROC curve is a graph that plots the true positive rate (TPR) against the false positive rate (FPR) at different classification thresholds.\n",
    "<br><br>\n",
    "AUC-ROC = Area under the ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e6823-e05a-423e-a65c-ae0039b0195a",
   "metadata": {},
   "source": [
    "# 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476f464-1495-490c-b6f0-a8780c4dbb17",
   "metadata": {},
   "source": [
    "- The accuracy of a model is one of the metrics that can be calculated from a confusion matrix, but it does not provide the full picture of the model's performance. \n",
    "- Accuracy is calculated as the ratio of the number of correct predictions to the total number of predictions made by the model. \n",
    "- It gives an overall measure of the model's ability to correctly classify instances.\n",
    "<br><br>\n",
    "Accuracy = (True Positive + True Negative) / (True Positive + False Positive + True Negative + False Negative)\n",
    "<br><br>\n",
    "Thus, the values in the confusion matrix like TP, FP, TN FN are used to calculate the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a6f595-4de0-4c6e-89d8-f4f8c5c75f43",
   "metadata": {},
   "source": [
    "# 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8945b709-2e44-4d62-98bc-d7cc4654d80d",
   "metadata": {},
   "source": [
    "A confusion matrix can be used to identify potential biases or limitations in a machine learning model by analyzing the performance of the model for each class of the target variable. Here are some ways to do so:\n",
    "\n",
    "1. Class distribution: \n",
    "- The class distribution in the target variable can be checked to identify if it is balanced or imbalanced. \n",
    "- If the distribution is imbalanced, it can lead to biased predictions for the minority class. \n",
    "- In such cases, accuracy may not be the right metric to evaluate the model's performance. \n",
    "- Instead, metrics like precision, recall, F1-score, or AUC-ROC can be used.\n",
    "\n",
    "2. Misclassification: \n",
    "- The confusion matrix can be analyzed to identify which classes are being misclassified by the model. \n",
    "- If a class is consistently being misclassified, it may indicate a limitation in the model's ability to capture the nuances of that class.\n",
    "\n",
    "3. Threshold optimization: \n",
    "- Depending on the business problem and cost of misclassification, the classification threshold can be optimized to maximize precision or recall. \n",
    "- For example, in fraud detection, the cost of false positives may be high, so a higher threshold can be used to increase precision.\n",
    "\n",
    "4. Confusion matrix heatmaps: \n",
    "- Confusion matrix heatmaps can be used to visualize the performance of the model for each class. \n",
    "- The heatmap can be color-coded to show the number of true positives, false positives, true negatives, and false negatives for each class. \n",
    "- This can help identify patterns and insights that are not immediately apparent from the raw numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374878f0-09bb-4a95-b087-bcc016ae427b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
