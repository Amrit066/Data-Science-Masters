{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a9f617-334a-45e6-8986-9d47d630aec9",
   "metadata": {},
   "source": [
    "# Assignment Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62954c16-8098-4cc1-8d37-82c3363fab78",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d30cb7-6d15-4349-ac9d-852e5495396e",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. \n",
    "- This method performs L2 regularization.  \n",
    "- It also helps in reducing the overfitting in the dataset.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "- When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values.\n",
    "- Ridge regression is a better predictor than least squares regression when the predictor variables are more than the observations. The least squares method cannot tell the difference between more useful and less useful predictor variables and includes all the predictors while developing a model. This reduces the accuracy of the model, resulting in overfitting and redundancy.\n",
    "- All of the above challenges are addressed by ridge regression. Ridge regression works with the advantage of not requiring unbiased estimators – rather, it adds bias to estimators to reduce the standard error. It adds bias enough to make the estimates a reliable representation of the population of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6826ef-4993-4900-a64f-7381c552c9fd",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d588b6-7991-4491-ba41-3c6650c4ab79",
   "metadata": {},
   "source": [
    "The assumptions of ridge regression are the same as that of linear regression: <br>\n",
    "- linearity \n",
    "- constant variance\n",
    "- independence. <br>\n",
    "However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5552436a-0f04-4061-b64f-9eb76ce78022",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd04a906-1df9-4058-b5f6-4c081ee57f58",
   "metadata": {},
   "source": [
    "We can use AIC or BIC to select λ for linear predictive models. BIC works better for very large sample sizes, but often picks up too simple models for small samples. <br>\n",
    "We can also use cross validation method to determine the apprpriate lambda value, using the steps given below:<br>\n",
    "1. Choose the number of folds K.\n",
    "2. Split the data accordingly into training and testing sets.\n",
    "3. Define a grid of values for lambda.\n",
    "4. For each lambda calculate the validation Mean Squared Error (MSE) within each fold.\n",
    "5. For each  lambda calculate the overall cross-validation MSE.\n",
    "6. Locate under which  lambda cross-validation MSE is minimised. \n",
    "7. This value of lambdais known as the minimum_CV  lambda. And it's the required lambda value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed44cfe-9448-4dd1-a68e-d499a08c9945",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb975f6a-7b08-46e9-b5da-57f16139ba6d",
   "metadata": {},
   "source": [
    "Yes, we can use ridge regression for feature selection while fitting the model. We cam achieve this by following the steps given below<br><br>\n",
    "1. We will try to find the coefficient of each feature using L2 normalization(or ridge regression technique).\n",
    "2. After finding the coefficients, we will check features with more importance using their coefficients whether they are positive or negative, or have high or low magnitude.\n",
    "3. Optionally, we can also plot the coefficients of each feature to determine the most important features.\n",
    "<br><br>\n",
    "So, in this way we can use Ridge regression in a way to selct features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111dc9f-cfdc-41d7-81a0-3463dc4828ac",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff2bf5-2b99-4aee-94eb-83ef7ab0c51c",
   "metadata": {},
   "source": [
    "- Multicollinearity happens when predictor variables exhibit a correlation among themselves. Ridge regression aims at reducing the standard error by adding some bias in the estimates of the regression. The reduction of the standard error in regression estimates significantly increases the reliability of the estimates.\n",
    "- Consider the straightforward situation of just two predictors (x1, x2). When fitting a plane to the data (y is the third dimension), there is frequently a very obvious \"best\" plane if there is no or little colinearity and good spread in both predictors. However, in the case of colinearity, the connection is actually a line through three dimensions with data strewn all around it. The choice of a plane depends on the important points in the data; if one of those points changes slightly, the \"best\" fitting plane will change significantly. However, the regression routine attempts to fit a plane to a line, so there are an infinite number of planes that perfectly intersect that line.\n",
    "What ridge regression does is pull the chosen plane towards simpler/saner models (bias values towards 0). T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fcba08-bbba-400d-ab86-2e28442ba32a",
   "metadata": {},
   "source": [
    "# 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cc92a1-d10d-4f45-9f79-b4c5a46896da",
   "metadata": {},
   "source": [
    "Ridge regression is a linear regression technique that can handle both categorical and continuous independent variables. Categorical variables need to be represented using appropriate coding schemes, such as one-hot encoding, before they can be included in the model.\n",
    "\n",
    "One-hot encoding is a common method used to represent categorical variables in Ridge regression. This involves creating a binary variable for each category of the categorical variable, where the binary variable takes a value of 1 if the observation belongs to that category, and 0 otherwise. These binary variables can then be used as predictor variables in the Ridge regression model.\n",
    "\n",
    "It is important to note that when using one-hot encoding to represent categorical variables, the number of predictor variables in the model can increase substantially, which may increase the risk of overfitting. Therefore, regularization is often necessary to prevent overfitting, and Ridge regression is a useful technique for this purpose. By adding a regularization term to the objective function, Ridge regression can effectively shrink the coefficients of predictor variables and prevent overfitting.\n",
    "\n",
    "In summary, Ridge regression can handle both categorical and continuous independent variables, but appropriate coding schemes, such as one-hot encoding, may be necessary for categorical variables. Additionally, regularization is often necessary to prevent overfitting when using Ridge regression with a large number of predictor variables, including those resulting from one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338800a8-df3e-4797-ab39-a555977505e6",
   "metadata": {},
   "source": [
    "# 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5486d-9198-468c-ae88-f8a02353bfa3",
   "metadata": {},
   "source": [
    "In Ridge Regression, the coefficients are estimated by minimizing the sum of squared errors, subject to a penalty term that is proportional to the square of the magnitude of the coefficients. The penalty term is determined by the regularization parameter, which is typically denoted by the symbol lambda.\n",
    "\n",
    "The Ridge Regression coefficients represent the expected change in the dependent variable for a one-unit increase in the corresponding independent variable, holding all other independent variables constant. However, due to the presence of the penalty term in the optimization objective, the Ridge Regression coefficients are typically smaller in magnitude than the coefficients estimated in non-regularized linear regression.\n",
    "\n",
    "The size of the Ridge Regression coefficients depends on the value of the regularization parameter lambda. As lambda increases, the magnitude of the coefficients decreases, and coefficients that are less important for predicting the dependent variable tend to shrink towards zero.\n",
    "\n",
    "The interpretation of the coefficients in Ridge Regression can be challenging because the Ridge penalty affects all the coefficients, and it can be difficult to determine which coefficients are most important in predicting the dependent variable. One approach to interpret the coefficients in Ridge Regression is to use the standardized coefficients, which are obtained by scaling the independent variables to have unit variance before fitting the model. The standardized coefficients represent the change in the dependent variable in terms of the number of standard deviations of the independent variable, which allows for a comparison of the relative importance of different independent variables.\n",
    "\n",
    "Thus, interpreting the coefficients of Ridge Regression involves considering the expected change in the dependent variable for a one-unit increase in the corresponding independent variable, while keeping other variables constant. However, the Ridge penalty affects all coefficients, making it challenging to determine which coefficients are most important. Standardized coefficients can be used to compare the relative importance of different independent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dab792-c6e9-4b76-9eb5-0d66de29a7d2",
   "metadata": {},
   "source": [
    "# 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db670f-c03e-4abe-8603-21f450d8e6ea",
   "metadata": {},
   "source": [
    "Ridge Regression can be used for time-series data analysis, but there are some important considerations to keep in mind.\n",
    "\n",
    "Firstly, time-series data typically exhibit autocorrelation, meaning that observations are correlated with their previous observations. This violates the assumption of independence of observations that is often made in Ridge Regression. Therefore, when using Ridge Regression with time-series data, it is important to account for the autocorrelation by using appropriate modeling techniques, such as autoregressive integrated moving average (ARIMA) models or state space models.\n",
    "\n",
    "Secondly, Ridge Regression assumes that the predictor variables are fixed and independent of time. This may not be appropriate for time-series data, where the values of the predictor variables may change over time. In such cases, it may be necessary to use dynamic regression models, such as autoregressive distributed lag (ARDL) models or vector autoregression (VAR) models, that allow for time-varying predictor variables.\n",
    "\n",
    "Finally, when using Ridge Regression with time-series data, it is important to consider the possibility of non-stationarity, meaning that the mean or variance of the time series changes over time. Non-stationarity can affect the performance of Ridge Regression, as the model assumes that the mean and variance of the dependent variable are constant over time. Therefore, it may be necessary to use techniques such as detrending or differencing to remove non-stationarity before applying Ridge Regression.\n",
    "\n",
    "Hence, we can say that Ridge Regression can be used for time-series data analysis, but it is important to account for autocorrelation, time-varying predictor variables, and non-stationarity. Appropriate modeling techniques, such as ARIMA models or dynamic regression models, may need to be used to address these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef428489-5f17-4afb-8b18-729746f751b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
