{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66b7678c-72bc-4ead-8419-ecbf2807c77b",
   "metadata": {},
   "source": [
    "# Assignment Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815338c6-fb8f-4053-8d81-3b3248cc3887",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4250ae9-8f74-4a12-ab35-23fc1756163d",
   "metadata": {},
   "source": [
    "1. Overfitting\n",
    "- Overfitting is the situation when the model performs better on training data but not on testind data.\n",
    "- In overfitting, the bias is low and variance is high.\n",
    "<br><br>\n",
    "Consequences of overfitting the model:<br>\n",
    "- The model will not be able to perform well on the data that it has not seen.\n",
    "- Also, its accuracy on prediction of new data will be much lower.\n",
    "<br><br>\n",
    "Mitigation of overfitting the data:<br>\n",
    "To avoid our model to be overfitted with the training dataset, we can follow the methods mentioned below;<br>\n",
    "- Cross-Validation\n",
    "- Training with more data\n",
    "- Removing features\n",
    "- Early stopping the training\n",
    "- Regularization\n",
    "- Ensembling\n",
    "2. Underfitting\n",
    "- Underfitting is the situation when the model does not perform better either on training data or on testind data.\n",
    "- In underfitting, both the bias and variance is high.\n",
    "<br><br>\n",
    "Consequences of underfitting the model:<br>\n",
    "- The model will not be able to learn properly on training dataset.\n",
    "- Hence, it will not be able to perform well on the data that it has not seen.\n",
    "- Also, its accuracy on prediction of new data will be much lower.\n",
    "- And it will not be able to give accurate answers on the training data either.\n",
    "<br><br>\n",
    "Mitigation of underfitting the data:<br>\n",
    "To avoid our model to be underfitted, we can follow the processes mentioned below;<br>\n",
    "- By increasing the training time of the model.\n",
    "- By increasing the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa889ec-a9a9-44fc-a6c2-7dfc6f864054",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e826a6ca-db32-4824-98db-d3eb7acb3fcd",
   "metadata": {},
   "source": [
    "Wecan follow the techniques mentioned below to prevent overfitting:\n",
    "<br><br>\n",
    "1. Early stopping: \n",
    "- This method seeks to pause training before the model starts learning the noise within the model. \n",
    "- This approach risks halting the training process too soon, leading to the opposite problem of underfitting. \n",
    "- Finding a common point between underfitting and overfitting is the ultimate goal here.\n",
    "2. Train with more data: \n",
    "- Expanding the training set to include more data can increase the accuracy of the model by providing more opportunities to parse out the dominant relationship among the input and output variables.\n",
    "- That said, this is a more effective method when clean, relevant data is injected into the model.\n",
    "3. Data augmentation: \n",
    "- A larger dataset would reduce overfitting. \n",
    "- If we cannot gather more data and are constrained to the data we have in our current dataset, we can apply data augmentation to artificially increase the size of our dataset.\n",
    "4. Feature selection: \n",
    "- Feature selection is the process of identifying the most important ones within the training data and then eliminating the irrelevant or redundant ones. \n",
    "5. Regularization: \n",
    "- If overfitting occurs when a model is too complex, it makes sense for us to reduce the number of features. But what if we don’t know which inputs to eliminate during the feature selection process? If we don’t know which features to remove from our model, regularization methods can be particularly helpful. \n",
    "- It applies a “penalty” to the input parameters with the larger coefficients, which subsequently limits the amount of variance in the model.  \n",
    "6. Ensemble methods: \n",
    "- Ensemble learning methods are made up of a set of classifiers and their predictions are aggregated to identify the most popular result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5537da5-c64d-43b4-9568-21dd3e865260",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07de28c5-9742-4764-9dd2-1c18af1c3ee7",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- Underfitting is the situation when the model does not perform better either on training data or on testind data.\n",
    "- In underfitting, both the bias and variance is high.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "The situation where underfitting can occur are;<br>\n",
    "- When data used for training is not cleaned and contains noise in it.\n",
    "- When the model has a high bias\n",
    "- When the size of the training dataset used is not enough.\n",
    "- When the model is too simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf192d4-e292-44b2-a1ef-0589eadc8215",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f095b0-41e9-4c3e-8887-c0fb6dffdf2a",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "- Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance.\n",
    "- When a we modify ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance. This way, the model will fit with the data set while increasing the chances of inaccurate predictions.\n",
    "- The same applies when creating a low variance model with a higher bias. While it will reduce the risk of inaccurate predictions, the model will not properly match the data set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a353b5f-9de3-4c59-87a0-f4c572784dbc",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc3663-f094-45d0-bf2f-11a9cbfa148e",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "Some ways in which we can determine if the model is overfitted or underfitted are discussed below:<br>\n",
    "- We can identify overfitting by looking at validation metrics, like loss or accuracy.\n",
    "- By using learning curve of a good fit model.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data.\n",
    "<br><br>\n",
    "- The model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples and the target values. \n",
    "- The model is overfitting the training data when the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5156d9e-1ef6-45b5-8f15-7f11816f0a4d",
   "metadata": {},
   "source": [
    "# 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a95f3-f15e-4888-acce-48cc5b10ad33",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "1. Bias\n",
    "- Bias is the error that calculates the difference between the average prediction of our model and the actual value that we are trying to predict. \n",
    "- A model suffering from high bias is a simple model which pays very little attention to the training data. This type of model always leads to a high error on both training and test data. \n",
    "2. Variance\n",
    "- Variance is the opposite of Bias. \n",
    "- It is also an error that measures the randomness of the predicted value from the actual value.\n",
    "- When we train our model with too much data or allow it to view the data too many times, it will learn the data including noise, which will cause our model to consider trivial features as important. And this will lead to high variance in the model.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "Some models with their level of biasness or variances are discussed below.<br><br>\n",
    "- Linear Regression\t\n",
    "    - It has high bias\tand low variance\n",
    "- Decision Tree\t\n",
    "    - It has low bias and high variance\n",
    "- Bagging\t\n",
    "    - It has low bias and high variance (Less than Decision Tree)\n",
    "\n",
    "- Random Forest\t\n",
    "    - It has has low bias and high variance (Less than Decision Tree and Bagging)\n",
    "<br>\n",
    "Performances of each of the ML algorithms increases in the above given chronological order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141648ac-6df5-424a-951c-40417ff7ab29",
   "metadata": {},
   "source": [
    "# 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29785be1-243d-4410-a2e3-9d8ccca04e31",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- The term \"regularization\" describes methods for calibrating machine learning models to reduce the modified loss function and avoid overfitting or underfitting.\n",
    "- In this technique, we reduce the magnitude of the features by keeping the same number of features.\n",
    "- It works by adding a penalty or complexity term to the complex model.\n",
    "<br>\n",
    "##### Part-2:<br><br>\n",
    "The method of regularization works by penalizing the parameter. And as we know that the parameters in an overfit model are typically inflated. Therefore regularization makes the criteria heavier and adds penalties while avoiding them. The cost function of the linear equation is increased by the factors. As a result, the cost function will grow if the parameter rises. And in order to reduce the cost function, the linear regression model will attempt to maximize the coefficient.<br>\n",
    "And, thus the model will work well instead of being overfitted.\n",
    "<br><br>\n",
    "##### Part-3:<br><br>\n",
    "There are mainly two types of regularization techniques, which are given below:<br>\n",
    "1. Ridge Regression\n",
    "- It is one of the types of linear regression in which a small amount of bias is introduced so that we can get better long-term predictions.\n",
    "- It is a regularization technique, which is used to reduce the complexity of the model. \n",
    "- It is also called as L2 regularization.\n",
    "- In this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty. We can calculate it by multiplying with the lambda to the squared weight of each individual feature.\n",
    "- A general linear or polynomial regression will fail if there is high collinearity between the independent variables, so to solve such problems, Ridge regression can be used.\n",
    "- It helps us to solve the problems if we have more parameters than samples.\n",
    "2. Lasso Regression\n",
    "- It stands for Least Absolute and Selection Operator.\n",
    "- It is similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights.\n",
    "- Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0.\n",
    "- It is also called as L1 regularization. \n",
    "- Some of the features in this technique are completely neglected for model evaluation. Hence, the Lasso regression can help us to reduce the overfitting in the model as well as the feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88802cd-d1bd-499b-9b83-3da75c3f7599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
