{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92c4376-0da2-4338-b93a-c41fb7e19d4b",
   "metadata": {},
   "source": [
    "# Assignment Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd54684-c3bc-4114-b582-b7bc0a602346",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc971078-c3b1-42e7-af13-726b1aa2c2e6",
   "metadata": {},
   "source": [
    "Simple linear regression is a machine learning algorithm that is used to find the relationship between two variables, in which one variable is called as independent variable and other is called as dependent variable.<br>\n",
    "Example: Predicting height of a person based on their weight.\n",
    "<br><br>\n",
    "While, multiple linear regression is an extended form of simple linear regression in which there are more than one independent variables are present.<br>\n",
    "Example: Predicting price of house based on number of room, its area, and location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef893d-06a3-4bbb-b04c-c957adb28a1d",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5b8b7-1ae6-456c-9314-6857db636552",
   "metadata": {},
   "source": [
    "The assumptions of linear regression are given below:<br><br>\n",
    "1. There should be a linear and additive relationship between dependent (response) variable and independent (predictor) variable(s).\n",
    "2. There should be no correlation between the residual (error) terms. Absence of this phenomenon is known as Autocorrelation.\n",
    "3. The independent variables should not be correlated. Absence of this phenomenon is known as multicollinearity.\n",
    "4. The error terms must have constant variance. This phenomenon is known as homoskedasticity. The presence of non-constant variance is referred to heteroskedasticity.\n",
    "5. The error terms must be normally distributed.\n",
    "<br><br>\n",
    "To check the presence of these assumptions in dataset are explained below respectively:<br><br>\n",
    "1. We can look for residual vs fitted value plots to check the linear dependency. \n",
    "2. We should look for Durbin – Watson (DW) statistic. It must lie between 0 and 4. If DW = 2, implies no autocorrelation, 0 < DW < 2 implies positive autocorrelation while 2 < DW < 4 indicates negative autocorrelation. Also, we can see residual vs time plot and look for the seasonal or correlated pattern in residual values.\n",
    "3. We can use scatter plot to visualize correlation effect among variables. Also, we can use VIF factor. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. Above all, a correlation table should also solve the purpose.\n",
    "4. We can look at residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern. Also, we can use Breusch-Pagan / Cook – Weisberg test or White general test to detect this phenomenon.\n",
    "5. We can look at QQ plot. We can also perform statistical tests of normality such as Kolmogorov-Smirnov test, Shapiro-Wilk test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f34330-7be0-45d4-ac44-3a3b3614f71d",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1267f9e1-1034-46f6-8366-7cd331cf7fec",
   "metadata": {},
   "source": [
    "- __Slope__ in a linear regression model shows the relationship between dependent and independent variable. It signifies the effect of unit change in independent variable on the dependent variable\n",
    "- __Intercept__ in a linear regression model tell us about the initial value of the dependent variable. That is it tells us whether the value that we are predicting starts from some numbers or it is zero.<br><br>\n",
    "Example: For example, suppose we want to predict the salary of employees working in acompany based on their level of experience.<br>\n",
    "Here, we know an employee can have 0,1,2,and so on years of experience. Also, for each year of experience there will be corresponding sdalary. <br>\n",
    "So, we can derive an equation as follows,<br>\n",
    "salary = init_sal + m * experience(say a simple linear regression equation)<br><br>\n",
    "Therefore, the salary corresponding to 0 years of experience i.e., init_sal from the above equation is an _intercept_. While, m in the above equation is a _slope_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217453d0-9a4e-40e8-b334-e848034faf0f",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a96d6-5bb6-4fc1-a8fc-860cec512387",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- Gradient Descent is defined as one of the most popular iterative optimisation algorithms in machine learning, gradient descent is used to teach both deep learning and machine learning models. It aids in locating a function's local minimum.\n",
    "- The best way to define the local minimum or local maximum of a function using gradient descent is as follows:\n",
    "\n",
    "    - If we move towards a negative gradient or away from the gradient of the function at the current point, it will give the local minimum of that function.\n",
    "    - Whenever we move towards a positive gradient or towards the gradient of the function at the current point, we will get the local maximum of that function.\n",
    "- This entire process is known as Gradient Ascent, which is also known as steepest descent. The primary objective of using a gradient descent algorithm is to minimise the cost function using iteration. It carries out the following two stages repeatedly to reach this goal:\n",
    "\n",
    "    - Determines the function's gradient or slope by computing the function's first-order derivative.\n",
    "    - Move away from the gradient's direction, which indicates that the slope has risen from the present point by an amount equal to alpha times, where alpha is the learning rate. It is a tuning parameter used in the optimisation process to help determine how long the steps should be.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "- Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks.  Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935dcc26-3dbf-4cf8-a632-140859d37263",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b37ae-f9a3-4132-a130-15729298e76b",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- Multiple linear regression is a machine learning algorithm which is used to train the model to predict the outcome of a dependent variable which needed more than one independent variable to do this task.\n",
    "- Formula:\n",
    "    - y = A+B1x1+B2x2+B3x3+B4x4\n",
    "    - where, A -  intercept; x1,x2,x3,x4, are independent variables\n",
    "    \n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "- A simple linear regression can accurately capture the relationship between two variables in simple relationships. On the other hand, multiple regression includes two or more independent variables – sometimes called predictor variables – in the model, rather than just one.\n",
    "\n",
    "- A multiple regression model uses more than one independent variable. It does not suffer from the same limitations as the simple regression equation, and it is thus able to fit curved and non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ca490-4842-4432-8541-69c9b66249e9",
   "metadata": {},
   "source": [
    "# 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d4372-ef44-4320-8727-fc155e2b72ab",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- Multicollinearity occurs when two or more independent variables in a data frame have a high correlation with one another in a regression model.\n",
    "- Multicollinearity is a problem because it will make the statistical inferences less reliable.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "- To detect multicollinearity in a regression model, we can use VIF factor.\n",
    "- The Variance Inflation Factor (VIF) can provide information about which variable or variables are redundant, and thus the variables that have a high VIF can be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fee39e-38b3-4109-8d6b-2b6a3233f657",
   "metadata": {},
   "source": [
    "# 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28278e9-ab30-4eda-a706-eb5fe747739b",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- Polynomial regression is a kind of linear regression in which the relationship shared between the dependent and independent variables Y and X is modeled as the nth degree of the polynomial. The fitting of a nonlinear connection between the value of x and the conditional mean of y is known as polynomial regression. It typically matched the least-squares approach. This form of linear regression involves fitting a polynomial equation to the data while the relationship between the dependent and independent variables is curvilinear.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "Polynomial regression is a form of Linear regression where only due to the Non-linear relationship between dependent and independent variables, we add some polynomial terms to linear regression to convert it into Polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c73fc92-93a9-4598-b3ee-e6ce417772dd",
   "metadata": {},
   "source": [
    "# 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328e8bf-5075-45fa-ac7a-671496c99c3c",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression.\n",
    "- A broad range of functions can easily fit under it.\n",
    "- The polynomial regression offers the best approximation of the relationship between the two dependent and independent variables.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "- Polynomial regression can be used when there is no linear correlation between the variables. Hence, it explains why it looks more like a non-linear function.\n",
    "- It can be used when linear regression models may not adequately capture the complexity of the relationship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6a561-e247-459e-b23e-b2d3ccf094ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
