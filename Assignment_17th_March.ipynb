{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e48f95-c3b9-4992-821b-f28df6792906",
   "metadata": {},
   "source": [
    "# Assignment Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e457d-5d37-43b0-a0be-06f3113a704d",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb3328-649e-4d3e-9e3f-20c0c29c101e",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "Missing value can be defined as a value for a variable in the dataset that is not sotred or captured in the dataset. \n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "Lack of handling missing values can lead to the following consequences:<br>\n",
    "- The model may not train upto the best with the available dataset, which can result in low prediction accuracy.\n",
    "- If important data in the dataset for a feture is missing, then model may get train with bias, which can lead to underfitting of the model.\n",
    "<br><br>\n",
    "##### Part-3:<br><br>\n",
    "K-nearest neighbour and Naive Bayes algorithm are some of the algorithms that are not effected by the dataset with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d263207-d92a-4f38-8162-6195d557e7fa",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915d8708-7abc-4dd4-9321-629cfd5122ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e361e0d4-90ec-4b6d-ac60-942a12bbe3a4",
   "metadata": {},
   "source": [
    "There are mainly two categories of techniques used to handle missind data, each of which can be classified further. These are;<br>\n",
    "1. Deleting the entire set\n",
    "- This method is used to delete the entire set of data either row-wise or column-wise.\n",
    "- It can be of two types.\n",
    "    1. Delete row\n",
    "    - This method is used when we have different missing values in a row.\n",
    "    - Then, it is used to delete the entire row data from the dataset.\n",
    "    - Eg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985eda49-e34c-4f8b-8956-bd3f8a4589e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "# Checking initial shape of the dataset\n",
    "print(\"Initial shape:\",titanic.shape)\n",
    "# Deleting rows with missing values\n",
    "new_titanic = titanic.dropna()\n",
    "# Checking shape of the dataset after row-wise deletion\n",
    "print(\"Final shap:\",new_titanic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc7bf37-81f6-4a7f-9f7f-87b2ea4983b3",
   "metadata": {},
   "source": [
    "    2. Delete column\n",
    "    - This method is used when we have different missing values in a column.\n",
    "    - Then, it is used to delete the entire column data from the dataset.\n",
    "    - Eg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826ae31-d505-4633-a4ca-36435c8520b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "# Checking initial shape of the dataset\n",
    "print(\"Initial shape:\",titanic.shape)\n",
    "# Deleting rows with missing values\n",
    "new_titanic = titanic.dropna(axis=1)\n",
    "# Checking shape of the dataset after row-wise deletion\n",
    "print(\"Final shap:\",new_titanic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3134d6-c77e-402b-b1c8-1d95ae9aea6e",
   "metadata": {},
   "source": [
    "2. Imputing values for the missing data\n",
    "- This method is used to fill the missing values with some other values.\n",
    "- Different techniques used in imputing methods are as follows.\n",
    "    1. Imputing mean value\n",
    "    - Here, we replace the missing values with the mean of the data in respective column.\n",
    "    - Eg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acfd944-60e7-47f3-87b5-8d2ce20bc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "# Checking for columns with missing data\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effcda69-5ccb-43ea-948e-049a269e935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will replace the age column with mean value\n",
    "titanic[\"new_age\"] =titanic[\"age\"].fillna(titanic[\"age\"].mean())\n",
    "# Checking new column with imputed data\n",
    "titanic[[\"new_age\",\"age\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7da46-76a5-4e38-be2a-303fda1efb73",
   "metadata": {},
   "source": [
    "    2. Imputing median value\n",
    "    - Here, we replace the missing values with the median of the data in respective column.\n",
    "    - Eg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fb086-e232-4cbe-927c-7f01235e285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "# Checking for columns with missing data\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c0868-d175-4553-a082-755b3f7e89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will replace the age column with median value\n",
    "titanic[\"new_age\"] =titanic[\"age\"].fillna(titanic[\"age\"].median())\n",
    "# Checking new column with imputed data\n",
    "titanic[[\"new_age\",\"age\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9150ad-f82a-49f8-8b7d-9971eb2404c0",
   "metadata": {},
   "source": [
    "    3. Imputing meode value\n",
    "    - Here, we replace the missing values with the mode of the data in respective column.\n",
    "    - It is generally used for categorical data.\n",
    "    - Eg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45efd14-7606-4a84-8ce6-0a369e8c86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "# Checking for columns with missing data\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73831e-eed6-4cc0-a77e-32ddf00525fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking type of data in the column\n",
    "# If it's string then the data will be of categorical type\n",
    "type(titanic[\"embarked\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca4b8a-81aa-4bcb-9be0-d862c64d5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will replace the embarked column with mode value\n",
    "titanic[\"new_embarked\"] =titanic[\"embarked\"].fillna(titanic[\"embarked\"].mode()[0])\n",
    "# Checking new column with imputed data\n",
    "titanic[[\"new_embarked\",\"embarked\"]]\n",
    "# Checking to verify if all the missing value has been imputed or not\n",
    "titanic[\"new_embarked\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103262c-a292-4044-b6da-d5992ce0abb8",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef585e-76e9-4b18-8eaa-5fcb110b3525",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "A dataset is said to be imbalanced dataset if the proportion of one class is relatively higher than the other or vice-versa.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "If the imbalanced data will not be handled properly then it may lead biasness in the model. This means that due to less available data for minority class, model will be trained more on majority class data than minority class, and this will overfit the model with the data of majority class. Thus, at the time of testing the performance of the model, we will observe that it is giving high accuracy for the data of majority class, which is not good for a general model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc6fca-6576-47e4-b267-85bc76b1f6c2",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8cd878-2d8d-424c-a4c9-0ca2f09d18f2",
   "metadata": {},
   "source": [
    "1. Up-sampling\n",
    "- It is the process of randomly duplicating observations from the minority class.\n",
    "- For eaxmple, let's suppose that we have a dataset containing categorical data with two class say male and female, where the data related to male is comparatively much higher than female.<br>\n",
    "So, in this case we can perform up-sampling on female class data to resample the whole data into a appropriate proportion.\n",
    "2. Down-sampling\n",
    "- It is the process of randomly removing observations from the majority class.\n",
    "- For eaxmple, let's suppose that we have a dataset containing categorical data with two class say male and female, where the data related to male is comparatively much higher than female.<br>\n",
    "So, in this case we can perform down-sampling on male class data to resample the whole data into a appropriate proportion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cded4f47-3861-4139-a970-d2a9e740e328",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a1531-2a69-4353-b522-37f0d6148638",
   "metadata": {},
   "source": [
    "##### Part-1: Data Augmentation<br><br>\n",
    "- Data augmentation is a technique that uses current data to create modified copies of datasets, which are then used to artificially increase the training set. \n",
    "- It involves making small adjustments to the information or creating new data points using deep learning.  \n",
    "<br><br>\n",
    "##### Part-2:SMOTE<br><br>\n",
    "- SMOTE stands for Synthetic Minority Oversampling Technique.\n",
    "- It is a statistical technique for increasing the number of data in the dataset in a balanced way. \n",
    "- It works by generating new instances from existing minority class values that we supply as input. \n",
    "- It does not change the number of majority cases.\n",
    "- The new data created are using interpolation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d96632-ed40-4b48-af78-3357e10d3df1",
   "metadata": {},
   "source": [
    "# 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52676a3-8e79-48a9-be69-1753ba1a4fd6",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "Outliers are those data points in a given dataset which are different from the majority of data present in the distribution.<br>\n",
    "These values are either too less or too more than most of the data present in the dataset.<br>\n",
    "These values are located at extreme ends of either side of the distribution.<br><br>\n",
    "##### Part-2:<br><br>\n",
    "We know that data is important part to develope any machine learning model. Because in the early stage of developing the model, we use data to train it and make it learn from them. Therefore, if we will not provide data correctly then our model may end up being inefficient.<br>\n",
    "And same happen in the case of providing dataset to a model to train having ouliers in it. The outliers present in the data make the model skewed that is, the model will learn giving weightage to the outliers even though most of the data are not coherent with outliers. And this will lead to wrong prediction of data during the testing phase making the model inefficient.<br>\n",
    "Therefore, it's necessary to deal with outliers appropriately before learning a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277c841-4998-4a35-8dc1-be7fe522715b",
   "metadata": {},
   "source": [
    "# 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad4669-3b33-4726-a1e1-47b46c8239d8",
   "metadata": {},
   "source": [
    "There are many ways present to handle missing data in a dataset. Some of these are discussed below:<br><br>\n",
    "1. Deleting the entire set\n",
    "- This method is used to delete the entire set of data either row-wise or column-wise.\n",
    "- It can be of two types.<br>\n",
    "    1. Delete row\n",
    "    - This method is used when we have different missing values in a row.\n",
    "    - Then, it is used to delete the entire row data from the dataset.<br>\n",
    "    2. Delete column\n",
    "    - This method is used when we have different missing values in a column.\n",
    "    - Then, it is used to delete the entire column data from the dataset.<br>\n",
    "2. Imputing values for the missing data\n",
    "- This method is used to fill the missing values with some other values.\n",
    "- Different techniques used in imputing methods are as follows.<br>\n",
    "    1. Imputing mean value\n",
    "    - Here, we replace the missing values with the mean of the data in respective column.<br>\n",
    "    2. Imputing median value\n",
    "    - Here, we replace the missing values with the median of the data in respective column.<br>\n",
    "    3. Imputing meode value\n",
    "    - Here, we replace the missing values with the mode of the data in respective column.\n",
    "    - It is generally used for categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26c979-3fb0-4d91-b93c-dd40d06f5a5e",
   "metadata": {},
   "source": [
    "# 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ea367-4b50-4aef-adcf-8154b4a00b67",
   "metadata": {},
   "source": [
    "To check for MAR missing data mechanism, we can follow the technique discussed below:<br><br>\n",
    "- Take each missing field and recode it as one if it is missing and zero otherwise to see if the data are MAR. Then regress each of the the other factors onto it using a logistic regression. We can conclude it as MAR if p-value is significant, which denotes a link between the regressor and missingness.\n",
    "- A second technique is to create dummy variables for whether a variable is missing and assigning the following values,<br><br>\n",
    "\n",
    "1 = missing<br>\n",
    "0 = observed<br>\n",
    "<br>\n",
    "We can then run t-tests and chi-square tests between this variable and other variables in the data set to see if the missingness on this variable is related to the values of other variables. If any relation is found then it is MAR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a3790-1775-4f4d-9b34-80ecdf9f372d",
   "metadata": {},
   "source": [
    "# 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95291913-73b6-4b97-9e47-49ad063fb4e3",
   "metadata": {},
   "source": [
    "Let's take the \"condition of interest\" as our positive class and \"no condition of interest\" as our negative class in this case.<br>\n",
    "Now, we will look at some of the performance metrics that we can use here, so that it help us to evaluate the performance of model in better way.<br><br>\n",
    "1. FPR\n",
    "- It stands for False Positive Rate.\n",
    "- It is calculated as FP/(TN + FP).<br><br>\n",
    "We can use this measure in this case as it will give us the idea about the prediction pattern of the model.<br>\n",
    "In this case it should be high as the dataset is skewed with positive class data here. <br>\n",
    "Hence, it signifies that this is not a good model.\n",
    "2. FNR\n",
    "- It stands for False Negative Rate.\n",
    "- It is calculated as FN/(TP + FN).<br><br>\n",
    "This measure will help us to know if the model is able to identify wrong value as wrong or not.<br>\n",
    "In this case it should be high as the dataset is skewed with positive class data here.So, most of the time will not be able to identify the negative class value. <br>\n",
    "Hence, it signifies that this is not a good model.\n",
    "3. AUC score\n",
    "- It represents the probability that a random positive example is positioned to the right of a random negative example. \n",
    "- It ranges in value from 0 to 1. \n",
    "- A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\n",
    "<br><br>\n",
    "This metric can also be used to know the true performance of the model.<br>\n",
    "In this case, it should be very low and represents that the model predicts correctly for the positive data only not for the negative one.<br>\n",
    "Hence, it signifies that this is not a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44e098-ce6b-428d-801c-29e060efd5fb",
   "metadata": {},
   "source": [
    "# 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb502bb-7c25-43b1-b889-a9bc270ea9d2",
   "metadata": {},
   "source": [
    "The method that we can used to balance the dataset and down-sample the majority class is given below:<br><br>\n",
    "1. Down-sampling/Decimation\n",
    "- It is the process of randomly removing observations from the majority class.\n",
    "- For eaxmple, let's suppose that we have a dataset containing categorical data with two class say satisfied and unsatisfied, where the data related to satisfied is comparatively much higher than unsatisfied.<br>\n",
    "So, in this case we can perform down-sampling on satisfied class data to resample the whole data into a appropriate proportion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9be428-23a9-4f93-a194-94ad9a42f4ea",
   "metadata": {},
   "source": [
    "# 11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced75a7-4922-4ec0-a6de-a367850125f9",
   "metadata": {},
   "source": [
    "The method that we can used to balance the dataset and up-sample the minority class is given below:<br><br>\n",
    "1. Up-sampling\n",
    "- It is the process of randomly duplicating observations from the minority class.\n",
    "- For eaxmple, let's suppose that we have a dataset containing categorical data with two class say satisfied and unsatisfied, where the data related to satisfied is comparatively much higher than unsatisfied.<br>\n",
    "So, in this case we can perform up-sampling on unsatisfied class data to resample the whole data into a appropriate proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621fb4aa-ce52-47a0-99e0-d589e9ffea2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
