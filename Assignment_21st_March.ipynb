{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d788538b-6116-463e-8da4-2d5aa6a9089a",
   "metadata": {},
   "source": [
    "# Assignment Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af284dd7-62f0-45eb-8d47-a3561e340d37",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb8f57-0acb-41b5-8974-69c23fda109e",
   "metadata": {},
   "source": [
    "1. Ordinal Encoding\n",
    "- It is an encoding technique where categorical data is converted into numerical data based on the intrinsinc order of unique categories.\n",
    "2. Label Encoding\n",
    "- It is an encoding technique where categorical data is converted into numerical data based on their alphabetical order or number of occurences of each categories.\n",
    "<br><br>\n",
    "Example: Suppose that we have a dataset related to diagnosis of a disease with different features in which one of the feature is degree of illness. And let this feature have value like, high,normal, and low.<br>\n",
    "Now, here we will use ordinal encoding as these values posses some kind of ordering that is high should have higher numerical value comparative to normal and normal should have higher numerical value compared to low.<br>\n",
    "Thus we need to choose ordinal encoding instead of label encoding which will assign numerical values based on theire occurrences in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2360ee-45b5-4f35-bd17-e362a6d8153b",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991728a-c996-4864-9a71-b1b011c31c88",
   "metadata": {},
   "source": [
    "- Target Guided Ordinal Encoding is used to convert categorical data based on their relationships with the target variable.\n",
    "- In this technique, we replace each category in the ategorical variable with a numerical value based on the mean or median of target variable for that category.\n",
    "- This creates a monotonic relationship between the categorical variable and the target variable, which can improve the predictive power of our model.\n",
    "<br><br>\n",
    "This encoding technique is useful when we have a categorical variable with a large number of unique categories, and we want to use this variable as a feature in our machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1edaf48-70ce-429a-a1f2-174e85be8a00",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe43815d-7d4a-48f1-aa4f-dc948e0e5cf4",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- Covariance is a mathematical tool that is used to find the relationships between two variable in statistical analysis. \n",
    "- The metric assesses the degree to which the variables change collectively. - It is basically a measurement of the variance between two variables, to put it another way.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "Covariance is very important in statistical analysis as it helps us to know whether the two different variables are related to each other or not. And if they are realted then what is the nature of their relationship, is it proportional or inverse.\n",
    "<br><br>\n",
    "##### Part-1:<br><br>\n",
    "We can follow the following steps in order to calculate covariance,<br>\n",
    "Step-1: Calculate mean for each of the two variables.<br>\n",
    "Step-2: Then , subtract mean from each of the values present in each variables.<br>\n",
    "Step-3: Then multiply the corresponding values obtained in Step-2.\n",
    "Step-4: Finally, add all the values obtained in Step-3 and divide it with the size of the sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a1b52-035d-46aa-81c5-23532524fa4f",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe24746-e77e-45b2-8375-2cd11fc0fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4984247-ea24-4f01-91b4-3849c91fd08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random data\n",
    "np.random.seed(50)\n",
    "color = np.random.choice([\"red\",\"green\",\"blue\"],10)\n",
    "size = np.random.choice([\"small\",\"medium\",\"large\"],10)\n",
    "material = np.random.choice([\"wood\",\"metal\",\"plastic\"],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3fe215-b532-4c98-8ce6-4cf01380708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Data\n",
    "le = LabelEncoder()  # creating object of label encoder\n",
    "\n",
    "color_encoded = le.fit_transform(color)\n",
    "size_encoded = le.fit_transform(size)\n",
    "material_encoded = le.fit_transform(material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf304e-60a0-4e05-9129-4cb5675a4896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datafram\n",
    "df = pd.DataFrame({\"color\":color,\"size\":size,\"material\":material})\n",
    "df_encoded = pd.DataFrame({\"color_encoded\":color_encoded,\"size_encoded\":size_encoded,\"material_encoded\":material_encoded})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acdb246-5f77-467b-8cb4-681f7103d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result\n",
    "print(\"Original Data\")\n",
    "print(df)\n",
    "print(\"Encoded Data\")\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab77b4-8371-4f77-b787-e60c32deb4ee",
   "metadata": {},
   "source": [
    "From the above output we can observe that the encoder has assign following numbers to each of the category for different features.<br><br>\n",
    "1. Color\n",
    "    - blue = 0\n",
    "    - green =1\n",
    "    - red = 2\n",
    "2. size\n",
    "    - large = 0\n",
    "    - medium = 1\n",
    "    - small = 2\n",
    "3. material\n",
    "    - metal = 0\n",
    "    - plastic = 1\n",
    "    - wood = 2\n",
    "<br><br>\n",
    "So, from the above observation it can be said that the number assigned by label encoder is based on the alphabetical order of the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a607b3-3329-4ea3-b7b7-391a1da61d66",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f00ce8-801f-456a-a2c8-d3c644d158d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea9a12-6636-4019-a527-5350b674ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Data\n",
    "np.random.seed(42)\n",
    "age = np.random.randint(20,100,10)\n",
    "income = np.random.randint(1,30,10)  # Numbers represent lakhs\n",
    "education_level = np.random.choice([\"isc\",\"matriculation\",\"ug\",\"pg\",\"phd\"],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec216e4-7925-47b6-85ca-04b19a046de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's at first encode educational _level using ordinal encodeing as it possess ordering of categories\n",
    "oe = OrdinalEncoder(categories= [[\"matriculation\",\"isc\",\"ug\",\"pg\",\"phd\"]])  # creating object of label encoder\n",
    "\n",
    "# oe.fit([[\"matriculation\",1],[\"isc\",2],[\"ug\",3],[\"pg\",4],[\"phd\",5]])\n",
    "education_level_encoded = np.array(pd.DataFrame(oe.fit_transform(pd.DataFrame({\"education_level\":education_level})[[\"education_level\"]]),columns=[\"education_level\"])[\"education_level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c8a376-0a18-4815-ab44-6503daa7e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "print(\"Original Education Level:\",education_level)\n",
    "print(\"Encoded Education Level:\",education_level_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11c3f0-9f90-4d79-9873-a5d517948354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will scale the data\n",
    "# Before that let's see if there are nay outliers presnt in any column\n",
    "df = pd.DataFrame({\"age\":age,\"income\":income,\"education_level\":education_level_encoded})\n",
    "\n",
    "# Checking outliers for age\n",
    "minm,q1,median,q3,mxm=np.quantile(np.array(df[\"age\"]),[0,0.25,0.5,0.75,1])\n",
    "iqr = q3-q1\n",
    "lower_limt= q1 - (iqr*1.5)\n",
    "upper_limt= q3 + (iqr*1.5)\n",
    "\n",
    "# Filtering data from age column which does not fall within range lower_limt to upper_limt\n",
    "if(df[(df[\"age\"]<lower_limt) & (df[\"age\"]>upper_limt)][\"age\"].size == 0):\n",
    "    print(\"There is no outliers in age\")\n",
    "else:\n",
    "    print(\"There is outlier in age\")\n",
    "    \n",
    "    \n",
    "# Checking outliers for income\n",
    "minm,q1,median,q3,mxm=np.quantile(np.array(df[\"income\"]),[0,0.25,0.5,0.75,1])\n",
    "iqr = q3-q1\n",
    "lower_limt= q1 - (iqr*1.5)\n",
    "upper_limt= q3 + (iqr*1.5)\n",
    "\n",
    "# Filtering data from age column which does not fall within range lower_limt to upper_limt\n",
    "if(df[(df[\"income\"]<lower_limt) & (df[\"income\"]>upper_limt)][\"income\"].size == 0):\n",
    "    print(\"There is no outliers in income\")\n",
    "else:\n",
    "    print(\"There is outlier in income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c668d2-9ff2-4b81-b74c-f325a6b7650b",
   "metadata": {},
   "source": [
    "From the above reult it is confirmed that neither of the column have any outliers present.<br>\n",
    "So, we can go with Min-Max Scaler for scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a8d18-c96d-4fbb-b4a6-1d55c05dbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "\n",
    "# For age\n",
    "mn = min(age)\n",
    "mx = max(age)\n",
    "age_scaled = np.array([round((i-mn)/(mx-mn),4) for i in age])\n",
    "\n",
    "# For income\n",
    "mn = min(income)\n",
    "mx = max(income)\n",
    "income_scaled = np.array([round((i-mn)/(mx-mn),4) for i in income])\n",
    "\n",
    "# For education level\n",
    "mn = min(education_level_encoded)\n",
    "mx = max(education_level_encoded)\n",
    "education_level_scaler = np.vectorize(lambda i:round((i-mn)/(mx-mn),4))\n",
    "education_level_scaled = education_level_scaler(education_level_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf36b06-9036-4e41-9744-813dae60e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe for scaled data\n",
    "df_scaled= pd.DataFrame({\"age_scaled\":age_scaled,\"income_scaled\":income_scaled,\"education_scaled\":education_level_scaled})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925c965-0edd-4ba3-aa4d-524499770adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"Scaled Data:\")\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499635cc-9b79-4ed0-952d-20ac83395fee",
   "metadata": {},
   "source": [
    "Finally, let's calculate the covariance matrix for all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a555d2-248c-40bc-a922-ff3f0e6bed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating covariance matrix\n",
    "cov_matrix = np.cov([age_scaled,income_scaled,education_level_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec760ef-da3f-4875-9e90-bc56d1b08484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Result\n",
    "print(\"Covariance Matrix:\\n\")\n",
    "print(cov_matrix);print()\n",
    "print(\"Covariance of age and income is {:.4f}\".format(cov_matrix[0][1]))\n",
    "print(\"Covariance of age and education level is {:.4f}\".format(cov_matrix[0][2]))\n",
    "print(\"Covariance of income and education level is {:.4f}\".format(cov_matrix[1][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b2512-7e8c-4cf0-ac16-530a5103aae7",
   "metadata": {},
   "source": [
    "From the above obtained covariance matrix we can see that,<br>\n",
    "there is a positive relationship between age and income but the magnitude of relationship is low.<br>\n",
    "Similiarly, there is a negative realtionship between age and education level, and income and education level.<br><br>\n",
    "__NOTE:__ Data were selected randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef992d5b-9546-4d09-b77c-fb20d29d89ed",
   "metadata": {},
   "source": [
    "# 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d72e73-a003-42a1-9769-d8cbe0a0b379",
   "metadata": {},
   "source": [
    "- For Gender feature, I will opt for One hot encoding. Because the number of unique features are less and we need both categories with equal importance for the analysis. Therefore, we can create a binary feature vector using one hot encoding for this category values.\n",
    "- For, Education level, we can choose Ordinal encoding , as ordinal encoding is applied on those categorical features which have intrinsic order or ranking. And here we can clearly see that the there will be a ranking associated with each unique categories to reflect the importance of each educational degree. Thus assigning the numerical value based on importance is needed. Therefore, we can go with ordinal encoding technique.\n",
    "- For this feature i.e., Employment Status, we can choose target guided ordinal encoding. Because target guided ordinal encoding helps us to perform encoding based on the relationship with target variable. And for feature like employment status, I think that its relationship with target variable should be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91d48b-ec88-4bc4-8bcd-2ef5aef0f702",
   "metadata": {},
   "source": [
    "# 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd5fe0-092f-484b-acfa-ad1bfc81f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869979d-f71b-4507-8d0b-89f35d7a0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Data\n",
    "np.random.seed(70)\n",
    "temperature = np.random.randint(10,40,10)\n",
    "humidity = np.random.randint(10,100,10)  # Numbers represent percentage\n",
    "weather = np.random.choice([\"sunny\",\"cloudy\",\"rainy\"],10)\n",
    "direction = np.random.choice([\"north\",\"south\",\"east\",\"west\"],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b77fd-d74a-4554-8ff5-f7a0379b84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting into dataframe\n",
    "df = pd.DataFrame({\"temperature\":temperature,\"humidity\":humidity,\"weather\":weather,\"direction\":direction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2580ca6f-d739-4ea0-87be-86abb32aaa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Categorical Data\n",
    "df_encoded = pd.get_dummies(df,columns= [\"weather\",\"direction\"])\n",
    "\n",
    "# Output\n",
    "print(\"Dataset with encoded value\")\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d5bb1-b73e-4e39-a502-9123ca34be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will scale the data\n",
    "# Before that let's see if there are nay outliers presnt in any column\n",
    "\n",
    "# Checking outliers for temperature\n",
    "minm,q1,median,q3,mxm=np.quantile(np.array(df_encoded[\"temperature\"]),[0,0.25,0.5,0.75,1])\n",
    "iqr = q3-q1\n",
    "lower_limt= q1 - (iqr*1.5)\n",
    "upper_limt= q3 + (iqr*1.5)\n",
    "\n",
    "# Filtering data from temperature column which does not fall within range lower_limt to upper_limt\n",
    "if(df[(df[\"temperature\"]<lower_limt) & (df[\"temperature\"]>upper_limt)][\"temperature\"].size == 0):\n",
    "    print(\"There is no outliers in temperature\")\n",
    "else:\n",
    "    print(\"There is outlier in temperature\")\n",
    "    \n",
    "    \n",
    "# Checking outliers for humidity\n",
    "minm,q1,median,q3,mxm=np.quantile(np.array(df[\"humidity\"]),[0,0.25,0.5,0.75,1])\n",
    "iqr = q3-q1\n",
    "lower_limt= q1 - (iqr*1.5)\n",
    "upper_limt= q3 + (iqr*1.5)\n",
    "\n",
    "# Filtering data from age column which does not fall within range lower_limt to upper_limt\n",
    "if(df[(df[\"humidity\"]<lower_limt) & (df[\"humidity\"]>upper_limt)][\"humidity\"].size == 0):\n",
    "    print(\"There is no outliers in humidity\")\n",
    "else:\n",
    "    print(\"There is outlier in humidity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace0033-d92f-4c51-b8d0-2bc31191ee85",
   "metadata": {},
   "source": [
    "From the above output, we can observe that there are no outliers in any of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370adaef-9b5b-4e70-ad63-a8ee18e864ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "\n",
    "# For temperature\n",
    "mn = min(np.array(df_encoded[\"temperature\"]))\n",
    "mx = max(np.array(df_encoded[\"temperature\"]))\n",
    "temperature_scaled = np.array([round((i-mn)/(mx-mn),4) for i in df_encoded[\"temperature\"]])\n",
    "\n",
    "# For humidity\n",
    "mn = min(np.array(df_encoded[\"humidity\"]))\n",
    "mx = max(np.array(df_encoded[\"humidity\"]))\n",
    "humidity_scaled = np.array([round((i-mn)/(mx-mn),4) for i in df_encoded[\"humidity\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cc48a-29c4-4998-87a9-2681b82a5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe for scaled data\n",
    "df_scaled= pd.concat([pd.DataFrame({\"temperature_scaled\":temperature_scaled,\"humidity_scaled\":humidity_scaled}),df_encoded[['weather_cloudy', 'weather_rainy',\n",
    "       'weather_sunny', 'direction_east', 'direction_north', 'direction_south',\n",
    "       'direction_west']]],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55eafb-2ea8-4f66-aaec-f3acdef4067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"Scaled Data:\")\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c648a4-bcf3-4435-9a98-ee190951028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating covariance matrix\n",
    "cov_matrix = np.cov([df_scaled['temperature_scaled'], df_scaled['humidity_scaled'], df_scaled['weather_cloudy'],\n",
    "       df_scaled['weather_rainy'], df_scaled['weather_sunny'], df_scaled['direction_east'], df_scaled['direction_north'],\n",
    "       df_scaled['direction_south'], df_scaled['direction_west']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d210d-e69d-4447-b81a-f357b6b50bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Result\n",
    "print(\"Covariance Matrix:\\n\")\n",
    "print(cov_matrix);print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
