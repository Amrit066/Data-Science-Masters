{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db75b10e-18f3-4377-8ca5-08ebb3a3346b",
   "metadata": {},
   "source": [
    "# Assignment Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a54625-044d-441a-ad5b-b47fac5b5b9d",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba2d76-30e7-4da5-95f7-e476d67395ec",
   "metadata": {},
   "source": [
    "##### Part-1:<br><br>\n",
    "- Filter methods is a collection of feature selection techniques which measure the relevance of features by their correlation with dependent variable.\n",
    "- Example: Fisher's Score, the Chi-Square Test, and information gain.\n",
    "<br><br>\n",
    "##### Part-2:<br><br>\n",
    "- In this approach, features are eliminated based on how they relate to or correlate with the result. \n",
    "- We use correlation to determine whether a feature's correlation to the output labels is positive or negative, and we remove features as necessary. - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2938ac-acc9-4d5a-b8fa-caf47eafa3fe",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6266659d-978d-4529-87d2-d0cdac52b2ab",
   "metadata": {},
   "source": [
    "The wrapper method differs from filter method in following way;<br><br>\n",
    "- It makes use of specified ML algorithms to obtain optimal features, while filter method provides generic set of methods.\n",
    "- It is comparatively low in terms of computational power than filter method.\n",
    "- In this approach, there is a high chance of overfitting, which is low in filter method.\n",
    "- It uses cross validation for evaluation of a subset of features, while Filter method uses statistical methods.\n",
    "- Wrapper methods can always provide the best subset of features, while filter method can fail in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007339b7-417b-4fb4-83af-227ce50d3184",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34cc44-fd67-4f51-9ef3-57f34ab0cd90",
   "metadata": {},
   "source": [
    "Embedded methods combine the best from filter and wrapping methods'. It is put into practice by programs with built-in feature selection techniques.<br>\n",
    "Regression techniques like LASSO and RIDGE, which have built-in penalization functions to minimize overfitting, are some of the most well-known examples of these techniques.<br>\n",
    "\n",
    "- L1 regularization is carried out during __lasso regression__, adding a penalty equal to the absolute value of the parameters' magnitude.\n",
    "- __Ridge regression__ employs L2 regularization, which imposes a penalty equal to the cube of the coefficients' magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3955ea-e32d-49be-b195-efdbdd5e204d",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9c1b4-c748-4651-8538-b5a3ca668129",
   "metadata": {},
   "source": [
    "Following are some drawbacks using filter methods for feature selection.<br><br>\n",
    "- This method may not give the optimal set of features.\n",
    "- They ignore how the classifier interacts with each feature, which results in feature relationships not being taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e45c5d-9cb9-44fc-a3df-c5280de5b426",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e7e36-9279-4b1a-be31-7c2067b9616f",
   "metadata": {},
   "source": [
    "When the dataset that we have contains large number of features, then we should use filter methods to perform feature selection as they are less computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0b9cd-2e30-486d-9cb5-3dba65b97835",
   "metadata": {},
   "source": [
    "# 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523e76a-da21-44d8-9d39-2b8844108690",
   "metadata": {},
   "source": [
    "We can utilize the power of following filter methods in order to choose appropriate features.<br><br>\n",
    "1. Correlation Coefficient: \n",
    "- It is a measure of linear relationship between two or more variables. \n",
    "- It can be used to select important features by checking that variables are correlated with target variables but not among each other.\n",
    "2. Chi-Square: \n",
    "- It is a is a statistical test applied to the groups of categorical features to evaluate the likelihood of correlation or association between them using their frequency distribution.\n",
    "- It will help us to remove those features which don't provide much of variance to the target variable\n",
    "3. Variance Threshold: \n",
    "- It removes all features whose variance does not meet some threshold.\n",
    "- We assume that feature with high variance may contain more useful information. Thus helping us to select only relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240793e7-9e8b-4160-8ed2-b85e7939d144",
   "metadata": {},
   "source": [
    "# 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42172bac-e32f-40da-ae51-93a1fe765a1b",
   "metadata": {},
   "source": [
    "We can use the following method in Embedded technique to select the relevant feature.<br><br>\n",
    "1. Regularization Approach:\n",
    "- LASSO(L1 Regularization)<br><br>\n",
    "The Lasso method puts a limitation/restrictions on the sum of the values of the model parameters.<br>\n",
    "The sum has to be less than the specific fixed value.<br>\n",
    "This Shrinks some of the coefficients to zero, Indicating that a certain predictor or certain features will be multiplied by zero to estimate the target.<br>\n",
    "During this process the variables that have non-zero co-efficient after shrinking are selected to be the part of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d6c8c3-5bae-45fc-9cd7-9a8ace6d8b8d",
   "metadata": {},
   "source": [
    "# 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0717544f-90dd-47c0-9160-13f08c8561dd",
   "metadata": {},
   "source": [
    "As the number of features here are not more then we can use the following given wrapper techniques to select features.<br><br>\n",
    "1. Repeated Ellimination of Features\n",
    "- It works by searching for a subset of features by starting with all features in the training dataset and successfully removing features until the desired number remains.\n",
    "- This is achieved by fitting the given machine learning algorithm used in the core of the model, ranking features by importance, discarding the least important features, and re-fitting the model. This process is repeated until a specified number of features remains.\n",
    "2. Exhaustive Feature Selection\n",
    "- The most reliable feature selection technique yet discussed is this one. - - Each feature subgroup is evaluated using a brute-force approach. The best-performing subset is returned after testing every conceivable combination of the variables, according to this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
